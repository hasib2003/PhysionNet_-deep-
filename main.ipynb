{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Raw_PhysionNet,PSD_PhysioNet\n",
    "from torch.utils.data import  random_split,DataLoader\n",
    "import torch\n",
    "from classifier import Deep_Classifier\n",
    "import os\n",
    "import config_local\n",
    "import torch.nn as nn\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataloader(dataset,split_ratios):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset: torch dataset\n",
    "        split_ratio: list of floats that sums to one, represeting the size of train ,val and test_set \n",
    "    \"\"\"\n",
    "\n",
    "    train,val,test = random_split(dataset,lengths=split_ratios)\n",
    "\n",
    "    train_loader = DataLoader(train,batch_size=32,shuffle=True)\n",
    "    val_loader = DataLoader(val,batch_size=32,shuffle=False)\n",
    "    test_loader = DataLoader(test,batch_size=32,shuffle=False)\n",
    "\n",
    "    return train_loader,val_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,val_loader,optimizer,criterion,num_epochs, loss_train, loss_val, acc_val):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "\n",
    "            batch_data = batch_data.float()\n",
    "            batch_labels = batch_labels.long()\n",
    "\n",
    "            # print(\"batch_labels \",batch_labels.shape)\n",
    "\n",
    "            if(batch_data.shape[0] == 1):\n",
    "                continue # skipping any batchsize with only one example since batch normalization is being used\n",
    "            # print(\"batch_data.shape 1\",batch_data.shape)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            epoch_loss+= loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # saving the epoch loss\n",
    "        epoch_loss = epoch_loss/len(train_loader)\n",
    "        loss_train.append(epoch_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data = batch_data.float()\n",
    "                batch_labels = batch_labels.long()\n",
    "                \n",
    "                outputs = model(batch_data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "                val_loss += criterion(outputs, batch_labels).item()\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            loss_val.append(val_loss)\n",
    "            acc_val.append(accuracy)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {accuracy:.2f}')\n",
    "    \n",
    "    return loss_train,loss_val,acc_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader,criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    prdicted_labels = [] # list of all predicted labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            outputs = model(batch_data.float())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "            val_loss += criterion(outputs, batch_labels.long()).item()\n",
    "\n",
    "            prdicted_labels = prdicted_labels + list(predicted) \n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        accuracy = correct / total\n",
    "    \n",
    "    \n",
    "    print(f'Test Loss: {val_loss:.4f}, Test Acc: {accuracy:.2f}')\n",
    "\n",
    "    return val_loss , accuracy , prdicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(vectors):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity matrix for a set of vectors.\n",
    "\n",
    "    Parameters:\n",
    "    vectors (torch.Tensor): A torch tensor of shape (n, 128) representing n vectors.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: An n x n matrix where element (i, j) represents the cosine similarity between vector i and vector j.\n",
    "    \"\"\"\n",
    "    # Normalize the vectors\n",
    "    norm_vectors = vectors / vectors.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute the cosine similarity matrix\n",
    "    similarity_matrix = torch.mm(norm_vectors, norm_vectors.T)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = torch.rand((50,128))\n",
    "mat = cosine_similarity_matrix(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7503)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_combinations(elements, length):\n",
    "    \"\"\"\n",
    "    Generates all possible combinations of a given length from a list of elements.\n",
    "\n",
    "    Parameters:\n",
    "    elements (list): The list of elements to combine.\n",
    "    length (int): The length of each combination.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of lists, where each inner list is a combination.\n",
    "    \"\"\"\n",
    "    return np.array(list(itertools.product(elements, repeat=length)))\n",
    "\n",
    "# Example usage\n",
    "elements = [1, 2]\n",
    "length = 2\n",
    "\n",
    "combinations = generate_combinations(elements, length)\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[tuple(combinations[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (2,) "
     ]
    }
   ],
   "source": [
    "np.array([1,2,3])-np.array([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low=1,high=10,size=(128,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 4], [2, 5], [3]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def group_indices(tensor):\n",
    "    groups = defaultdict(list)\n",
    "    for idx, item in enumerate(tensor):\n",
    "        groups[item.item()].append(idx)\n",
    "    return list(groups.values())\n",
    "\n",
    "# Example usage\n",
    "input_tensor = torch.tensor([2, 2, 3, 1, 2, 3])\n",
    "result = group_indices(input_tensor)\n",
    "print(result)  # Output: [[0, 3], [1, 4], [2, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SUBJS = int(os.getenv(\"NUM_SUBJS\"))\n",
    "bin_dict_extnd = {\n",
    "    \"delta\":[1,4],\n",
    "    \"theta\":[4,8],\n",
    "    \"alpha\":[8,13],\n",
    "    \"beta\":[13,30],\n",
    "    \"gamma\":[30,79]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deep_Classifier(num_classes=NUM_SUBJS)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... found 1526 edf files ....\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = Raw_PhysionNet()\n",
    "# psd_dataset = PSD_PhysioNet(raw_dataset=raw_dataset,freq_bin=bin_dict_extnd,duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4706, 64, 321)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.eeg_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4706, 64, 321)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.eeg_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18824"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,val_loader,test_loader =gen_dataloader(raw_dataset,[0.7,0.15,0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  3,  2, 12,  3,  2,  9,  2,  4,  6,  9, 12,  8, 11,  0, 11, 11,  1,\n",
      "         0,  0,  2,  2, 11, 10, 11,  7,  2,  6,  7, 12,  3,  8])\n"
     ]
    }
   ],
   "source": [
    "for i,j in train_loader:\n",
    "    print(j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train=[]\n",
    "loss_val=[]\n",
    "acc_val=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 80])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep_Classifier(\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (point_wise): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (max_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=6144, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc5): Linear(in_features=128, out_features=13, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n",
      "out.shape  torch.Size([32, 6144])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mloss_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43macc_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, num_epochs, loss_train, loss_val, acc_val)\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m     21\u001b[0m     epoch_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# saving the epoch loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,train_loader,val_loader,optimizer,criterion,1000,loss_train,loss_val,acc_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
